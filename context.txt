CODEBASE CONTEXT â€” YouTube RAG Backend (Node + Python)
1ï¸âƒ£ Project Identity

Project Type
YouTube Question-Answering system using RAG (Retrieval Augmented Generation).

What it does

Takes a YouTube URL + user question

Extracts transcript

Chunks + embeds transcript

Stores embeddings per video

Answers questions strictly from video content

Returns timestamps for answers

Manages video lifecycle with retries & auto-repair

Maintains per-video chat memory with summarization

Tech Stack

Node.js (Express) â†’ API gateway / orchestration

Python (FastAPI) â†’ Core RAG engine

LangChain + Chroma

LLMs via Groq (OpenAI-compatible)

Embeddings (currently Gemini, designed to be swappable)

SQLAlchemy + DB for state & chat memory

2ï¸âƒ£ High-Level Architecture
Client
  â”‚
  â–¼
Node.js API
  â”‚
  â–¼
Python FastAPI
  â”‚
  â”œâ”€ Video State Gate
  â”‚
  â”œâ”€ Background Video Processing
  â”‚     â”œâ”€ Transcript Fetch
  â”‚     â”œâ”€ Chunking
  â”‚     â”œâ”€ Embeddings
  â”‚     â””â”€ Vector Store (per video)
  â”‚
  â””â”€ RAG Query Flow
        â”œâ”€ Retriever (MMR)
        â”œâ”€ Deduplication
        â”œâ”€ Re-ranking
        â”œâ”€ Token limiting
        â”œâ”€ Formatting with timestamps
        â””â”€ LLM Answer

3ï¸âƒ£ Directory Tree (Condensed)
backend/                 # Node.js
â”œâ”€â”€ index.js
â”œâ”€â”€ routes/
â”‚   â””â”€â”€ query.route.js
â””â”€â”€ utils/
    â”œâ”€â”€ youtubeUrl.utils.js
    â””â”€â”€ languageDetection.utils.js

python-backend/
â”œâ”€â”€ main.py
â”œâ”€â”€ config.py
â”œâ”€â”€ routes/
â”‚   â”œâ”€â”€ ask_query_route.py
â”‚   â”œâ”€â”€ ingest_route.py
â”‚   â””â”€â”€ status_route.py
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ core_rag/
â”‚   â”œâ”€â”€ application_services/
â”‚   â”œâ”€â”€ video_repair/
â”‚   â””â”€â”€ memory/
â”œâ”€â”€ db/
â”‚   â”œâ”€â”€ database.py
â”‚   â”œâ”€â”€ models.py
â”‚   â””â”€â”€ chat_memory_schema.py
â””â”€â”€ vector_store/

4ï¸âƒ£ Node.js Backend Responsibilities
query.route.js

Validates input

Extracts videoId

Detects video language

Forwards request to Python /query

youtubeUrl.utils.js

Safely extracts YouTube video ID

Supports watch, shorts, embed, youtu.be

languageDetection.utils.js

Scrapes YouTube HTML

Extracts languageCode

Defaults to "en"

5ï¸âƒ£ Python Routes Responsibilities
/query

Entry point for asking questions

Delegates to state handler

Executes RAG only when video is READY

/process-video

Manual ingest endpoint

Pulls FAILED videos and reprocesses them

/video-status/{video_id}

Used by frontend polling

Returns NOT_FOUND | PROCESSING | READY | FAILED

6ï¸âƒ£ Video Lifecycle & State Machine

States

NOT_FOUND â†’ PROCESSING â†’ READY
                    â†˜
                     FAILED â†’ retry (max 2) â†’ permanent FAILED


Stored in DB

video_id

status

languages

error

retry_count

7ï¸âƒ£ Core RAG Pipeline (core_rag)
Transcript

Uses youtube_transcript_api

Language-aware

Throws explicit errors

Chunking

Adaptive chunk size based on transcript length

Preserves timestamps per chunk

Embeddings

Central factory (create_embeddings)

Designed to swap providers (OpenAI, Gemini, Cohere, Voyage, etc.)

Vector Store

One Chroma index per video

Path: vector_store/{videoId}

Skip processing if embeddings already exist

Retrieval

MMR retriever

fetch_k = 12, k = 6

Filters

Deduplicate repeated chunks

Enforce max context size

Re-ranking

CrossEncoder (ms-marco-MiniLM)

Improves relevance before LLM

Formatting

Injects [start_time - end_time] into context

Returns docs for timestamp extraction

Prompt Rules

Answer ONLY from transcript

No hallucination

No meta commentary

Polite, professional, concise

8ï¸âƒ£ Application Services (Orchestration Layer)
video_processing_service

Full ingestion pipeline

Sets state â†’ PROCESSING / READY / FAILED

query_state_handler_service

Gatekeeper before RAG

Handles retries

Schedules background processing

query_rag_executor_service

Validates question

Loads chat memory

Executes RAG chain

Formats final response with timestamps

response_formatter_service

Cleans LLM output

Formats timestamps (frontend-friendly)

9ï¸âƒ£ Chat Memory System

Stored per (user + video)

Components

Repository â†’ DB persistence

Manager â†’ orchestration

Formatter â†’ readable history

Summarizer â†’ compress history when large

Token estimator â†’ heuristic guardrails

Behavior

Summarizes when token threshold crossed

Rejects queries if chat is too long

Preserves essential context only

ğŸ” Auto-Repair System

Background scheduler runs every 30 minutes

Picks FAILED videos

Marks PROCESSING

Re-runs ingestion

Max retry = 2

ğŸ” Config Philosophy

config.py is environment-driven

.env is never shared

Supports multiple LLM / embedding providers

âš ï¸ Important Invariants (Rules)

One video = one vector store

Never answer without READY state

Never hallucinate beyond transcript

Non-commented code = production truth

Node = gateway, Python = intelligence

Frontend polls state, not blocks

ğŸ§  How to Use This Context in Future Chats

In a new chat, paste this entire block and say:

â€œUse this as full backend context.â€

Then ask:

â€œReview my RAG designâ€

â€œHelp me refactor embeddingsâ€

â€œAdd multi-user supportâ€

â€œOptimize retrievalâ€

â€œDesign frontend contractâ€

No re-sharing needed.